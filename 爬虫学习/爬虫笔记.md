
# 爬虫笔记

## 基础知识

1. **概念**：模拟客户端（主要指浏览器）发送网络请求，接受请求响应，并按照一定规则自动抓取信息的程序

2. **作用**：
   1. 数据采集
   2. 软件测试
   3. 抢票，投票
   4. 网络安全
  
3. **分类**：
   1. 数量 ：通用；聚焦爬虫
   2. 是否以获取数据为目的：功能性；数据增量爬虫：url不变数据改变；url改变数据改变。

4. **流程**：![爬虫流程](E:\\爬虫流程.png)

5. **http**
   1. http协议：超文本传输协议，默认端口80
   2. https：http+SSL（安全套接字层，对传输内容进行加密）默认端口；443.

6. **请求头**：
   1. Content-Type
   2. 2.host 域名
   3. 3.Connection 长连接
   4. 4.Upgr-Insecure-Requs 升级为https请求
   5. User-Agent 用户代理，提供系统信息和浏览器信息
   6. 6.Referer 页面跳转处
   7. 7.Cookie 本地数据（状态保持）

7. **响应头**(常用)：Set-Cookie 对方服务器设置cookie到本地浏览器的缓存

8. **常见响应状态码**（对爬虫来说都不可信）：
   + 200成功
   + 302跳转
   + 303浏览器对于POST请求的响应重新定向值新的url
   + 307浏览器对于GET请求的响应重新定向值新的url
   + 403资源不可用；服务器理解用户的请求但拒绝处理（无权限）
   + 404 找不到页面 500服务器内部错误
   + 503服务器由于维护或者负载过重无法应答，在响应中可能携带Retry-After响应头；或者爬虫频繁访问urlnetwork中抓包得到的源码为判断依据，而elements中的源码为渲染之后的，不能作为判断依据。

9. **浏览器运行过程**:
   浏览器发送所有请求，并渲染。爬虫一次请求一次回应，不进行渲染。
   抓包：骨骼文件：html静态文件；肌肉文件：js/ajax请求；皮肤文件：css/font(字体)/图片。
   抓包过程：对抓取的包进行分析，获取所需信息。

## requests模块

**参考文档**： > <https://requests.readthedocs.io/en/master/user/quickstart/>

1. **作用**：发送请求，获取响应数据。

2. **get请求及显示请求内容**  
   + response.get发送get请求。可添加请求头字典修改默认请求头。  
   + response.text打印str类型源码，可使用+response.encoding手动设定编码类型。
   + response.content是bytes类型的相响应源码。可以response.content.decode()操作解决中文乱码。

3. **response响应对象的其他常用属性或方法**
   + response.url 响应的url
   + response.status_code 响应状态码
   + response.request.headers 响应对应的请求头
   + response.headers 响应头
   + response.requests._cookies 响应对象请求的cookie；返回cookieJar类型
   + response.cookies 响应的cookie（set-cookie后的），返回cookieJar类型
   + response.json() 自动将json字符串类型的响应内容转换为python对象（dict/list）

4. **发送带参数的请求**
   1. 直接在url中携带
   2. 通过params携带参数字典
   + url中'?'后面时参数 可逐个（多个）删除确定关键参数

5. **发送带cookie的请求**(需登录的网页需要使用的重要参数,cookie过期需重新获取)
   1. 直接在headers中带参数(即在headers字典中增加cookie键值对)
   2. 通过cookies参数携带cookies字典
      + 键值对：' cookie的name' : 'cookie的value'
      + cookie字符串    temp = {}
      稳妥：

        ```常用
            cookie_list = temp.split('; ')
            cookies = {}
            for cookie in cookie_list:
              cookies[cookie.split('=')[0]] = cookies[cookie.split('=')[-1]]
        ```

      字典推导式：

        ```秀技术
            cookies = {cookie.split('=')[0]: cookie.split('=')[-1] for cookie in temp.split('; ')}
        ```

   3. cookieJar对象转换为cookie字典
      方法：

      ```方法
      cookies_dict = requests.utils.dict_from_cookiejar(response.cookies)
      ```

      response.cookies为返回的cookieJar类型
      requests.utils.dict_from_cookiejar函数 返回字典类型
      转回方法：

        ```back
        cookie_jar = requests.utils.cookiejar_from_dict(cookie_dict)
        ```

6. **超时参数timeout的使用**
    使用方法：response = requests.get(url, timeout=n)
    n秒后强制返回结果

7. **代理的使用过程**
     1. 理解：网上的一个IP，指向代理服务器
     2. 分类：正向代理（明确请求服务器的IP） 反向代理（不知IP，如 nginx）
     3. 代理IP的分类：
         1. 按匿名度：
            1. 透明代理
            2. 匿名代理
            3. 高匿代理（效果最好,无法识别是否为代理，也无法识别个人ip）
         2. 按协议：
            4. http
            5. https
            6. socks

     4. proxies(dict)代理参数的使用：

      ```eg
      response = requests.get(url, proxies=proxies)
      proxies = {
      "http(s)": " http(s)://地址串:端口号",
      }
      ```
  
    注：使用失败时：1.卡滞 2.报错

8. **使用verify参数忽略CA证书**
  verify=False （会警告，不影响）

9. **requests发送post请求**（登录注册，发送大量文本时一般发送post请求）
     1. 方法：response = requests.post(url, data)
        + data 参数应接受dict
        + json库中的json.loads()将json字符串转化为字典
     2. post数据来源
        1. 固定值 通过抓包比较不变值
        2. 输入值 抓包比较自身变化值
        3. 预设值 - 静态文件值 需提前从html中获取
        4. 预设值 - 发请求 需对指定地址发送请求获取
        5. 在客户端生成  分析js 模拟生成数据
10. **requests.session进行状态保持**
      1. 作用：自动处理连续多次请求过程中产生的cookie
      2. 方法：

       ```eg
      session = requests.session() #实例化生成session对象
      response = session.get()
      response = session.post()
       ```

## 数据提取

1. **响应分类**：
    1. 结构化数据：
        1. json数据(高频出现)：提取方式：json模块 re模块 jsonpath模块
        2. xml数据：提取方式：re模块 lxml模块
    2. 非结构化数据：html字符串 提取方式：re模块 lxml模块
2. **xml和html区别**
   xml传输存储数据
   html展示数据
3. **常用数据解析方法**
   1. 结构化数据：
      1. json：
         1. json模块
         2. jsonpath模块
      2. xml：
         1. re模块 正则语法
         2. lxml模块 xpath语法
   2. 非结构化数据：
      1. html：
         1. re模块
         2. lxml模块
         3. beautifulsoup模块：
            1. xpath
            2. 正则
            3. css选择器
         4. pyquery css选择器
4. **jsonpath模块**
   1. 使用场景：按照key对python字典(多个嵌套的复杂字典)进行批量数据提取(结果为列表)
   2. 使用方法：一般先将返回的json字符串利用json.loads()转化为json格式

      ```eg
      from jsonpath import jsonpath
      ret = jsonpath(dict对象,'语法规则字符串')
      ```

   3. 常用语法：$ :根节点(最外层的大括号)
               . :子节点
               .. :内部任意位置，子孙节点 $..key 返回dict中key的value列表
5. **lxml模块**
   1. 简介：lxml模块使用xpath语法，快速定位html/xml文档中特定元素提取节点信息（文本 内容 属性）
   2. xpath节点选择语法：
       + 节点名字：选中节点 html
       + / 绝对路径 html/head/title 隔开节点
       + //相对路径 html//title
       + . 选中当前节点
       + ..选中当前父节点
       + @ 选取属性  //link/@href
       + text() 选择文本 //title/text() 从开闭标签中提取文本内容
   3. xpath节点修饰语法(选取特定节点)：
       + 通过索引修饰节点 eg：/html/body/div[2] /div[last()-1]选择倒数第二个 /div[position()>10]选中大于10的标签
       + 属性值修饰 eg：//div[@id='content-left']/div/@id @在[]中使用属性值修饰 结尾的/@+属性值是取属性值
       + 子节点值修饰 eg：//div[span[2]>=9.6] span为div的子节点
       + 部分包含修饰 eg1：//div[contains(@id,"qiushi_")] 匹配div标签中id属性值包含双引号里面内容的所有标签 eg2：//span[contains(text(),"下一页")] 匹配span标签中含有文本"下一页"的所有标签
   4. xpath选取未知节点语法(通配语法)
       + *匹配所有元素节点 eg：//*[@id="content"] 匹配所有id值为content的节点
       + @* 匹配所有属性节点
       + node() 匹配任何类型的节点
   5. xpath复选语法 | 或连接 eg：//h2/a|//td/a
   6. lxml模块使用：

       ```eg
         from lxml import etree
         html = etree.HTML(text) #html字符串转化为具有xpath方法的Element对象
         ret_list = html.xpath("语法字符串") #结果返回list，若返回的list中仍为Element对象则可以继续使用xath方法
      ```

      补：zip()函数，lxml.etree.tostring()函数可将Element转化为html字符串
